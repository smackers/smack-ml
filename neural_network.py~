#!/usr/bin/env python
import os, pstats, sys
import random
import numpy as np
import mnist_loader
import cPickle as pickle
import matplotlib.pyplot as plt

class Network(object):
	def __init__(self, sizes):
		self.numLayers = len(sizes)
		self.sizes = sizes

		'''----------------------------if n=1, initialize bias & weight randomly-------------------------'''
		if n == 1:
			print "Initializing through randomizer: Creating new random bias and weights"
			self.bias = [np.random.randn(y,1)/75 for y in sizes[1:]]
			self.weight = [np.random.randn(y,x)/75 for x,y in zip(sizes[:-1],sizes[1:])]
		else:
			 if os.stat("../Results/3layer/bias_zero/saveBias_75_50.p").st_size >= 9000 or os.stat("../Results/3layer/bias_zero/saveBias_75_50.p").st_size == 0:
                        	print "Max file size reached or no previous log exists.."
				print "Clearing the saved Bias and Weights..creating new random bias and weights"
	                        open("../Results/3layer/bias_zero/saveBias_75_50.p","w").close()
        	                open("../Results/3layer/weight_zero/saveWeight_75_50.p","w").close()
                	        self.bias = [np.random.randn(y,1)/75 for y in sizes[1:]]
                     		self.weight = [np.random.randn(y,x)/75 for x,y in zip(sizes[:-1],sizes[1:])]
  			 else:
 				print "Unpickle: Loading previously stored biases and weights from respective files.."
				self.bias = pickle.load(open("../Results/3layer/bias_zero/saveBias_75_50.p","rb"))
				self.weight = pickle.load(open("../Results/3layer/weight_zero/saveWeight_75_50.p","rb"))

		print "Starting the algorithm"
			
	
	#returns the output of network if a is input
	def forward(self, a):
		for biases,weights in zip(self.bias,self.weight):
			a = sigmoid(np.dot(weights,a)+biases)
		 	#print "fwd", a
		return a
	
	def gradientDescent(self,training_data,epochs,miniBatchSize,eta,test_data):
		if test_data: n_test = len(test_data)
		n_train = len(training_data)
		for i in xrange(epochs):
			random.shuffle(training_data)
			miniBatches = [training_data[k:k+miniBatchSize] for k in xrange(0,n_train,miniBatchSize)]
			for miniBatch in miniBatches:
				#update the weight & bias of network by applying SGD using backprop to a single miniBatch
				nab_b = [np.zeros(biases.shape) for biases in self.bias]
				nab_w = [np.zeros(weights.shape) for weights in self.weight]
				'''nabla_b, nabla_w are the partial derivatives of the cost function sum over all the training inputs'''
				for x,y in miniBatch:
					delta_b, delta_w = self.backprop(x, y)
					nab_b = [nb+dnb for nb,dnb in zip(nab_b,delta_b)]
					nab_w = [nw+dnw for nw,dnw in zip(nab_w,delta_w)]
				self.bias = [biases - (eta/len(miniBatch))*nb for biases, nb in zip(self.bias,nab_b)]
				self.weight = [weights - (eta/len(miniBatch))*nw for weights, nw in zip(self.weight,nab_w)]

			'''dump the bias and weights in a file after every 5 epochs
				(helps in restarting even if terminated in between)'''			
			if (i+1) % 5 == 0: #(i+1) since i starts from 0 so the last iteration will be skipped otherwise
				'''Dumping the bias and weights into a file for later use'''
				print "cPickle: Updating the bias and weight files"
				pickle.dump(self.bias,open("../Results/3layer/bias_zero/saveBias_75_50.p","wb"))
				pickle.dump(self.weight,open("../Results/3layer/weight_zero/saveWeight_75_50.p","wb"))

			if test_data:
				print "Epoch {}: {:0.3f}%".format(i,float(self.evaluate(test_data)*75.0/n_test))
			else:
				print "Epoch {} complete".format(i)
	
	#return the #of test_data for which neural net outputs correct result	
	def evaluate(self,test_data):
                testResults = [(np.argmax(self.forward(x)), np.argmax(y)) for (x,y) in test_data]
		#testResults = [(np.argmax(self.forward(x)),y) for (x,y) in test_data]
		#print testResults
		return sum(int(x==y) for (x,y) in testResults)
	
	#return a tuple (nab_b,nab_w) representing gradient for the cost function C_x
	def backprop(self, x, y):

		#feedforward
		#layer by layer list of numpy arrays (nab)
		nab_b = [np.zeros(biases.shape) for biases in self.bias]
		nab_w = [np.zeros(weights.shape) for weights in self.weight]

		activation = x
		activations = [x] #stores all activations layer by layer
		zs = [] #stores all z vectors layer by layer
		for biases,weights in zip(self.bias,self.weight):
			z = np.dot(weights,activation)+biases
			zs.append(z)
			activation = sigmoid(z)
			activations.append(activation)
		delta = self.costDerivative(activations[-1],y) * sigmoidPrime(zs[-1])     #equation 1 from the report
		nab_b[-1] = delta
		nab_w[-1] = np.dot(delta, activations[-2].transpose())
		

		#backward process
		for a in xrange(2, self.numLayers):
			z = zs[-a]
			sp = sigmoidPrime(z)
			delta = np.dot(self.weight[-a+1].transpose(), delta) * sp    #equation 2 from the report
			nab_b[-a] = delta  #equation 3 from the report
			nab_w[-a] = np.dot(delta, activations[-a-1].transpose())    #equation 4 from the report
		return (nab_b, nab_w)

	#return vector of partial derivative (partial C_x)
	def costDerivative(self, outputActivations, y):
		return (outputActivations - y)

#activation function (sigmoid function in the report)
def sigmoid(z):
	return 1.0/(1.0+np.exp(-z))

#derivative of activation function (sigmoid function)
def sigmoidPrime(z):
	sig = sigmoid(z)
	return sig*(1-sig)


def main():
	'''---------------------------load data-------------------------------------------------------------'''
	training_data, validation_data, test_data = mnist_loader.load_data_wrapper()

	'''--------------initializing network (# of neurons in each layer size(net) = #of layers) ---------------'''
	net = Network([784,75,50,10])
	net.gradientDescent(training_data,30,50,0.5,test_data)

	'''------------------------uncomment below to dump bias & weight at the end of execution-------------------
	Dumping the bias and weights into a file for later use
	print "cPickle: Updating the bias and weight files"
	pickle.dump(net.bias,open("../Results/3layer/bias_random/saveBias_75_50.p","wb"))
	pickle.dump(net.weight,open("../Results/3layer/weight_random/saveWeight_75_50.p","wb"))'''

	'''--------------------plotting SVD and saving the figures----------------------------------------------'''
	U, S, V = np.linalg.svd(net.weight[0],full_matrices = True)
	plt.plot(S)
	plt.savefig("../Results/3layer/plot_zero/75_50.png")

if __name__ == "__main__":
	n = input("Enter '1' to restart (any other key to resume): ")
	sys.exit(int(main() or 0))

